"""
Module de gestion du mod√®le de langage avec l'API Groq.

Ce module g√®re l'interface avec l'API Groq pour g√©n√©rer des r√©ponses
en utilisant le mod√®le Mixtral-8x7B-32768, optimis√© pour la rapidit√©
et l'efficacit√©.
"""

import logging
from typing import List, Dict, Any, Optional
from groq import Groq
# from langchain.llms.base import LLM
from langchain_core.language_models import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
#from langchain.callbacks.manager import CallbackManagerForLLMRun
from pydantic import BaseModel, Field

from .config import config

# Configuration du logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GroqLLM(LLM, BaseModel):
    """
    Impl√©mentation LangChain du mod√®le Groq.
    
    Cette classe encapsule l'API Groq dans une interface compatible
    avec LangChain pour une int√©gration transparente.
    """
    
    client: Optional[Groq] = Field(default=None)
    model_name: str = Field(default=config.LLM_MODEL)
    temperature: float = Field(default=0.1)
    max_tokens: int = Field(default=2048)
    top_p: float = Field(default=0.9)
    
    class Config:
        arbitrary_types_allowed = True
    
    def __init__(self, **kwargs):
        """Initialise le mod√®le Groq."""
        super().__init__(**kwargs)
        self._initialize_client()
    
    def _initialize_client(self) -> None:
        """
        Initialise le client Groq.
        
        Raises:
            Exception: Si la cl√© API n'est pas configur√©e.
        """
        if not config.GROQ_API_KEY:
            raise Exception(config.ERROR_MESSAGES["missing_api_key"])
        
        try:
            self.client = Groq(api_key=config.GROQ_API_KEY)
            logger.info(f"‚úÖ Client Groq initialis√© avec le mod√®le {self.model_name}")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du client Groq: {e}")
            raise Exception(f"Erreur d'initialisation Groq: {str(e)}")
    
    @property
    def _llm_type(self) -> str:
        """Retourne le type de LLM."""
        return "groq"
    
    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
        """
        Appelle le mod√®le Groq avec un prompt.
        
        Args:
            prompt (str): Prompt √† envoyer au mod√®le.
            stop (Optional[List[str]]): Mots d'arr√™t.
            run_manager (Optional[CallbackManagerForLLMRun]): Gestionnaire de callbacks.
            **kwargs: Arguments suppl√©mentaires.
            
        Returns:
            str: R√©ponse g√©n√©r√©e par le mod√®le.
            
        Raises:
            Exception: Si l'appel √©choue.
        """
        if self.client is None:
            raise Exception("Client Groq non initialis√©")
        
        try:
            logger.info(f"G√©n√©ration de r√©ponse avec Groq...")
            
            # Pr√©parer les param√®tres
            generation_params = {
                "model": self.model_name,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": self.temperature,
                "max_tokens": self.max_tokens,
                "top_p": self.top_p,
                "stream": False
            }
            
            # Ajouter les mots d'arr√™t si sp√©cifi√©s
            if stop:
                generation_params["stop"] = stop
            
            # Appeler l'API
            response = self.client.chat.completions.create(**generation_params)
            
            # Extraire la r√©ponse
            if response.choices and len(response.choices) > 0:
                generated_text = response.choices[0].message.content
                logger.info(f"‚úÖ R√©ponse g√©n√©r√©e ({len(generated_text)} caract√®res)")
                return generated_text
            else:
                raise Exception("Aucune r√©ponse g√©n√©r√©e par le mod√®le")
                
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration: {e}")
            raise Exception(f"Erreur de g√©n√©ration: {str(e)}")


class LLMHandler:
    """
    Gestionnaire principal du mod√®le de langage.
    
    Cette classe fournit une interface simplifi√©e pour interagir
    avec le mod√®le Groq et g√®re la configuration des prompts.
    """
    
    def __init__(self):
        """Initialise le gestionnaire LLM."""
        self.llm = None
        self._initialize_llm()
    
    def _initialize_llm(self) -> None:
        """
        Initialise le mod√®le de langage.
        
        Raises:
            Exception: Si l'initialisation √©choue.
        """
        try:
            self.llm = GroqLLM()
            logger.info("‚úÖ Gestionnaire LLM initialis√©")
            
        except Exception as e:
            logger.error(f"Erreur lors de l'initialisation du LLM: {e}")
            raise
    
    def generate_response(self, prompt: str) -> str:
        """
        G√©n√®re une r√©ponse √† partir d'un prompt.
        
        Args:
            prompt (str): Prompt d'entr√©e.
            
        Returns:
            str: R√©ponse g√©n√©r√©e.
            
        Raises:
            Exception: Si la g√©n√©ration √©choue.
        """
        if self.llm is None:
            raise Exception("LLM non initialis√©")
        
        try:
            return self.llm._call(prompt)
            
        except Exception as e:
            logger.error(f"Erreur lors de la g√©n√©ration: {e}")
            raise Exception(config.ERROR_MESSAGES["llm_error"])
    
    def create_rag_prompt(self, question: str, context_documents: List[str]) -> str:
        """
        Cr√©e un prompt optimis√© pour le RAG.
        
        Args:
            question (str): Question de l'utilisateur.
            context_documents (List[str]): Documents de contexte.
            
        Returns:
            str: Prompt format√© pour le RAG.
        """
        # Concat√©ner les documents de contexte
        context = "\n\n".join(context_documents)
        
        prompt = f"""Tu es un assistant sp√©cialis√© dans l'analyse de documents scientifiques. 
Tu dois r√©pondre aux questions en te basant uniquement sur les documents fournis.

CONTEXTE (Documents sources):
{context}

QUESTION: {question}

INSTRUCTIONS:
1. R√©ponds de mani√®re pr√©cise et d√©taill√©e en te basant uniquement sur les informations fournies dans le contexte.
2. Si l'information n'est pas disponible dans les documents, indique-le clairement.
3. Cite les sources en mentionnant le nom du document et le num√©ro de page quand c'est possible.
4. Structure ta r√©ponse de mani√®re claire et professionnelle.
5. Utilise un langage scientifique appropri√©.

R√âPONSE:"""
        
        return prompt
    
    def create_summary_prompt(self, documents: List[str]) -> str:
        """
        Cr√©e un prompt pour r√©sumer les documents.
        
        Args:
            documents (List[str]): Documents √† r√©sumer.
            
        Returns:
            str: Prompt format√© pour le r√©sum√©.
        """
        context = "\n\n".join(documents)
        
        prompt = f"""Tu es un expert en analyse de documents scientifiques. 
R√©sume les documents suivants en identifiant les points cl√©s, les m√©thodologies, 
et les conclusions principales.

DOCUMENTS:
{context}

R√âSUM√â (structurez avec des sections claires):"""
        
        return prompt
    
    def test_connection(self) -> Dict[str, Any]:
        """
        Teste la connexion avec l'API Groq.
        
        Returns:
            Dict[str, Any]: R√©sultat du test avec informations de statut.
        """
        try:
            if self.llm is None:
                return {
                    'status': 'error',
                    'message': 'LLM non initialis√©'
                }
            
            # Test simple
            test_prompt = "Dis 'Hello' en une phrase."
            response = self.generate_response(test_prompt)
            
            return {
                'status': 'success',
                'message': 'Connexion r√©ussie',
                'model': self.llm.model_name,
                'test_response': response,
                'response_length': len(response)
            }
            
        except Exception as e:
            return {
                'status': 'error',
                'message': f'Erreur de connexion: {str(e)}'
            }
    
    def get_model_info(self) -> Dict[str, Any]:
        """
        R√©cup√®re les informations sur le mod√®le configur√©.
        
        Returns:
            Dict[str, Any]: Informations sur le mod√®le.
        """
        if self.llm is None:
            return {
                'status': 'not_initialized',
                'model_name': None
            }
        
        return {
            'status': 'ready',
            'model_name': self.llm.model_name,
            'temperature': self.llm.temperature,
            'max_tokens': self.llm.max_tokens,
            'top_p': self.llm.top_p
        }
    
    def update_parameters(self, **kwargs) -> None:
        """
        Met √† jour les param√®tres du mod√®le.
        
        Args:
            **kwargs: Param√®tres √† mettre √† jour (temperature, max_tokens, etc.).
        """
        if self.llm is None:
            raise Exception("LLM non initialis√©")
        
        try:
            for key, value in kwargs.items():
                if hasattr(self.llm, key):
                    setattr(self.llm, key, value)
                    logger.info(f"Param√®tre {key} mis √† jour: {value}")
                else:
                    logger.warning(f"Param√®tre inconnu: {key}")
                    
        except Exception as e:
            logger.error(f"Erreur lors de la mise √† jour des param√®tres: {e}")
            raise


# Instance globale du gestionnaire
llm_handler = LLMHandler()


if __name__ == "__main__":
    # Test du module
    print("Test du module LLMHandler...")
    
    try:
        # Test de connexion
        test_result = llm_handler.test_connection()
        print(f"üîó Test de connexion: {test_result['status']}")
        
        if test_result['status'] == 'success':
            print(f"ü§ñ Mod√®le: {test_result['model']}")
            print(f"üìù R√©ponse test: {test_result['test_response']}")
        else:
            print(f"‚ùå Erreur: {test_result['message']}")
        
        # Informations du mod√®le
        model_info = llm_handler.get_model_info()
        print(f"\nüìä Informations du mod√®le:")
        print(f"  - Statut: {model_info['status']}")
        if model_info['status'] == 'ready':
            print(f"  - Mod√®le: {model_info['model_name']}")
            print(f"  - Temp√©rature: {model_info['temperature']}")
            print(f"  - Max tokens: {model_info['max_tokens']}")
            
    except Exception as e:
        print(f"‚ùå Erreur: {e}")
